{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of segmentation_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pabvald/Vision-Project-Image-Segmentation/blob/pablo-task1/Vision_task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6IxWe_R328L"
      },
      "source": [
        "# Image Segmentation Task 1\n",
        "#### Welcome to the first task of Image Segmentation. Image segmentation is the process of partitioning the image into a set of pixels representing an object. In this task, you will be introduced to the problem of image segmentation and programming pipeline involved in image segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlRautc2328S"
      },
      "source": [
        "For the purpose of this task we will be using PASCAL VOC datset. The dataset contains a total of 2913 images with segmentation annotations. Code in the cell below will download the code and extract the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GB5rBnC4hjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee06380-16af-4da4-a922-90ac6b8cdae1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_t4c-S3k31",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "!tar -xvf VOCtrainval_11-May-2012.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lvs9XIpBaI0",
        "outputId": "b8dc1c20-1785-4ff5-a848-40e19fedd048"
      },
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOqzj2BY328T"
      },
      "source": [
        "### 1.1 Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs8VbesPBqHL"
      },
      "source": [
        "import os\n",
        "from os.path import join as pjoin\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import imageio\n",
        "import numpy as np\n",
        "import scipy.misc as m\n",
        "import scipy.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision import models"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgZkqQc1BsdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd9a95b-10de-4b17-8402-e9160522165a"
      },
      "source": [
        "%config Completer.use_jedi = False\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei34r4MhB9-o"
      },
      "source": [
        "#### pascalVOCDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgKQvA6u6eka"
      },
      "source": [
        "class pascalVOCDataset(data.Dataset):\r\n",
        "    \"\"\"Data loader for the Pascal VOC semantic segmentation dataset.\r\n",
        "\r\n",
        "    Annotations from both the original VOC data (which consist of RGB images\r\n",
        "    in which colours map to specific classes) and the SBD (Berkely) dataset\r\n",
        "    (where annotations are stored as .mat files) are converted into a common\r\n",
        "    `label_mask` format.  Under this format, each mask is an (M,N) array of\r\n",
        "    integer values from 0 to 21, where 0 represents the background class.\r\n",
        "\r\n",
        "    The label masks are stored in a new folder, called `pre_encoded`, which\r\n",
        "    is added as a subdirectory of the `SegmentationClass` folder in the\r\n",
        "    original Pascal VOC data layout.\r\n",
        "\r\n",
        "    A total of five data splits are provided for working with the VOC data:\r\n",
        "        train: The original VOC 2012 training data - 1464 images\r\n",
        "        val: The original VOC 2012 validation data - 1449 images\r\n",
        "        trainval: The combination of `train` and `val` - 2913 images\r\n",
        "        train_aug: The unique images present in both the train split and\r\n",
        "                   training images from SBD: - 8829 images (the unique members\r\n",
        "                   of the result of combining lists of length 1464 and 8498)\r\n",
        "        train_aug_val: The original VOC 2012 validation data minus the images\r\n",
        "                   present in `train_aug` (This is done with the same logic as\r\n",
        "                   the validation set used in FCN PAMI paper, but with VOC 2012\r\n",
        "                   rather than VOC 2011) - 904 images\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        root,\r\n",
        "        sbd_path=None,\r\n",
        "        split=\"train_aug\",\r\n",
        "        is_transform=False,\r\n",
        "        img_size=512,\r\n",
        "        augmentations=None,\r\n",
        "        img_norm=True,\r\n",
        "        test_mode=False,\r\n",
        "    ):\r\n",
        "        self.root = root\r\n",
        "        self.sbd_path = sbd_path\r\n",
        "        self.split = split\r\n",
        "        self.is_transform = is_transform\r\n",
        "        self.augmentations = augmentations\r\n",
        "        self.img_norm = img_norm\r\n",
        "        self.test_mode = test_mode\r\n",
        "        self.n_classes = 21\r\n",
        "        self.mean = np.array([104.00699, 116.66877, 122.67892])\r\n",
        "        self.files = collections.defaultdict(list)\r\n",
        "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\r\n",
        "\r\n",
        "        if not self.test_mode:\r\n",
        "            for split in [\"train\", \"val\", \"trainval\"]:\r\n",
        "                path = pjoin(self.root, \"ImageSets/Segmentation\", split + \".txt\")\r\n",
        "                file_list = tuple(open(path, \"r\"))\r\n",
        "                file_list = [id_.rstrip() for id_ in file_list]\r\n",
        "                self.files[split] = file_list\r\n",
        "            self.setup_annotations()\r\n",
        "\r\n",
        "        self.tf = transforms.Compose(\r\n",
        "            [\r\n",
        "                # add more trasnformations as you see fit\r\n",
        "                transforms.ToTensor(),\r\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.files[self.split])\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        im_name = self.files[self.split][index]\r\n",
        "        im_path = pjoin(self.root, \"JPEGImages\", im_name + \".jpg\")\r\n",
        "        lbl_path = pjoin(self.root, \"SegmentationClass/pre_encoded\", im_name + \".png\")\r\n",
        "        im = Image.open(im_path)\r\n",
        "        lbl = Image.open(lbl_path)\r\n",
        "        if self.augmentations is not None:\r\n",
        "            im, lbl = self.augmentations(im, lbl)\r\n",
        "        if self.is_transform:\r\n",
        "            im, lbl = self.transform(im, lbl)\r\n",
        "        return im, torch.clamp(lbl, max=20)\r\n",
        "\r\n",
        "    def transform(self, img, lbl):\r\n",
        "        if self.img_size == (\"same\", \"same\"):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            img = img.resize((self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\r\n",
        "            lbl = lbl.resize((self.img_size[0], self.img_size[1]))\r\n",
        "        img = self.tf(img)\r\n",
        "        lbl = torch.from_numpy(np.array(lbl)).long()\r\n",
        "        lbl[lbl == 255] = 0\r\n",
        "        return img, lbl\r\n",
        "\r\n",
        "    def get_pascal_labels(self):\r\n",
        "        \"\"\"Load the mapping that associates pascal classes with label colors\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            np.ndarray with dimensions (21, 3)\r\n",
        "        \"\"\"\r\n",
        "        return np.asarray(\r\n",
        "            [\r\n",
        "                [0, 0, 0],\r\n",
        "                [128, 0, 0],\r\n",
        "                [0, 128, 0],\r\n",
        "                [128, 128, 0],\r\n",
        "                [0, 0, 128],\r\n",
        "                [128, 0, 128],\r\n",
        "                [0, 128, 128],\r\n",
        "                [128, 128, 128],\r\n",
        "                [64, 0, 0],\r\n",
        "                [192, 0, 0],\r\n",
        "                [64, 128, 0],\r\n",
        "                [192, 128, 0],\r\n",
        "                [64, 0, 128],\r\n",
        "                [192, 0, 128],\r\n",
        "                [64, 128, 128],\r\n",
        "                [192, 128, 128],\r\n",
        "                [0, 64, 0],\r\n",
        "                [128, 64, 0],\r\n",
        "                [0, 192, 0],\r\n",
        "                [128, 192, 0],\r\n",
        "                [0, 64, 128],\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "    def encode_segmap(self, mask):\r\n",
        "        \"\"\"Encode segmentation label images as pascal classes\r\n",
        "\r\n",
        "        Args:\r\n",
        "            mask (np.ndarray): raw segmentation label image of dimension\r\n",
        "              (M, N, 3), in which the Pascal classes are encoded as colours.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            (np.ndarray): class map with dimensions (M,N), where the value at\r\n",
        "            a given location is the integer denoting the class index.\r\n",
        "        \"\"\"\r\n",
        "        mask = mask.astype(int)\r\n",
        "        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\r\n",
        "        for ii, label in enumerate(self.get_pascal_labels()):\r\n",
        "            label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\r\n",
        "        label_mask = label_mask.astype(int)\r\n",
        "        # print(np.unique(label_mask))\r\n",
        "        return label_mask\r\n",
        "\r\n",
        "    def decode_segmap(self, label_mask, plot=False):\r\n",
        "        \"\"\"Decode segmentation class labels into a color image\r\n",
        "\r\n",
        "        Args:\r\n",
        "            label_mask (np.ndarray): an (M,N) array of integer values denoting\r\n",
        "              the class label at each spatial location.\r\n",
        "            plot (bool, optional): whether to show the resulting color image\r\n",
        "              in a figure.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            (np.ndarray, optional): the resulting decoded color image.\r\n",
        "        \"\"\"\r\n",
        "        label_colours = self.get_pascal_labels()\r\n",
        "        r = label_mask.copy()\r\n",
        "        g = label_mask.copy()\r\n",
        "        b = label_mask.copy()\r\n",
        "        for ll in range(0, self.n_classes):\r\n",
        "            r[label_mask == ll] = label_colours[ll, 0]\r\n",
        "            g[label_mask == ll] = label_colours[ll, 1]\r\n",
        "            b[label_mask == ll] = label_colours[ll, 2]\r\n",
        "        rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\r\n",
        "        rgb[:, :, 0] = r / 255.0\r\n",
        "        rgb[:, :, 1] = g / 255.0\r\n",
        "        rgb[:, :, 2] = b / 255.0\r\n",
        "        if plot:\r\n",
        "            plt.imshow(rgb)\r\n",
        "            plt.show()\r\n",
        "        else:\r\n",
        "            return rgb\r\n",
        "\r\n",
        "    def setup_annotations(self):\r\n",
        "        \"\"\"Sets up Berkley annotations by adding image indices to the\r\n",
        "        `train_aug` split and pre-encode all segmentation labels into the\r\n",
        "        common label_mask format (if this has not already been done). This\r\n",
        "        function also defines the `train_aug` and `train_aug_val` data splits\r\n",
        "        according to the description in the class docstring\r\n",
        "        \"\"\"\r\n",
        "        sbd_path = self.sbd_path\r\n",
        "        target_path = pjoin(self.root, \"SegmentationClass/pre_encoded\")\r\n",
        "        if not os.path.exists(target_path):\r\n",
        "            os.makedirs(target_path)\r\n",
        "        train_aug = self.files[\"train\"]\r\n",
        "\r\n",
        "        # keep unique elements (stable)\r\n",
        "        train_aug = [train_aug[i] for i in sorted(np.unique(train_aug, return_index=True)[1])]\r\n",
        "        self.files[\"train_aug\"] = train_aug\r\n",
        "        set_diff = set(self.files[\"val\"]) - set(train_aug)  # remove overlap\r\n",
        "        self.files[\"train_aug_val\"] = list(set_diff)\r\n",
        "\r\n",
        "        pre_encoded = glob.glob(pjoin(target_path, \"*.png\"))\r\n",
        "        expected = np.unique(self.files[\"train_aug\"] + self.files[\"val\"]).size\r\n",
        "\r\n",
        "        if len(pre_encoded) != expected:\r\n",
        "            print(\"Pre-encoding segmentation masks...\")\r\n",
        "\r\n",
        "            for ii in tqdm(self.files[\"trainval\"]):\r\n",
        "                fname = ii + \".png\"\r\n",
        "                lbl_path = pjoin(self.root, \"SegmentationClass\", fname)\r\n",
        "                lbl = self.encode_segmap(m.imread(lbl_path))\r\n",
        "                lbl = m.toimage(lbl, high=lbl.max(), low=lbl.min())\r\n",
        "                m.imsave(pjoin(target_path, fname), lbl)\r\n",
        "\r\n",
        "        assert expected == 2913, \"unexpected dataset sizes\""
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYHA2piV328X"
      },
      "source": [
        "### 1.2 Define the model architecture(2.0 point)\n",
        "In this section you have the freedom to decide your own model. Keep in mind though, to perform image segmentation, you need to implement an architecture that does pixel level classification i.e. for each pixel in the image you need to predict the probability of it belonging to one of the 21 categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBOheC2O4Ng9"
      },
      "source": [
        "#### Factory methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQv7OTvpIf84"
      },
      "source": [
        "def segnet_encoder_layer(in_channels, out_channels, kernel_size=3, \n",
        "                         stride=1, padding=1): \n",
        "  \"\"\"Create a SegNet encoder layer: Convolution + Batch Normalization + ReLU.\n",
        "\n",
        "    Args:\n",
        "      in_channels (int): number of input channels\n",
        "      out_channels (int): number of output channels\n",
        "      kernel_size (int): kernel size of the  convolution\n",
        "      stride (int): stride of the convolution\n",
        "      padding (int): padding of the convolution\n",
        "\n",
        "    Returns: \n",
        "      (torch.nn.Sequential): SegNet encoder layer\n",
        "  \"\"\"\n",
        "  layer = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
        "                stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU()\n",
        "  )\n",
        "  return layer \n",
        "\n",
        "def segnet_encoder_block(in_channels, out_channels):\n",
        "  \"\"\"Generate a SegNet encoder block ommitting the final Max Pooling.\n",
        "  \"\"\"\n",
        "  assert len(in_channels) == len(out_channels)\n",
        "  layers = [] \n",
        "  for i in range(len(in_channels)): \n",
        "    layer = segnet_encoder_layer(in_channels[i], out_channels[i])\n",
        "    layers.append(layer)\n",
        "  block = nn.Sequential(*layers)\n",
        "  return block\n",
        "\n",
        "def segnet_decoder_layer(in_channels, out_channels, kernel_size=3, \n",
        "                         stride=1, padding=1):\n",
        "  \"\"\"Create a SegNet decoder layer: Deconvolution + Batch Normalization + ReLU.\n",
        "\n",
        "    Args:\n",
        "      in_channels (int): number of input channels\n",
        "      out_channels (int): number of output channels\n",
        "      kernel_size (int): kernel size of the transpose convolution\n",
        "      stride (int): stride of the transpose convolution\n",
        "      padding (int): padding of the transpose convolution\n",
        "\n",
        "    Returns:\n",
        "      (torch.nn.Sequential): SegNet decoder layer\n",
        "  \"\"\"\n",
        "  layer = nn.Sequential(\n",
        "      nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, \n",
        "                stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU()\n",
        "  )\n",
        "  return layer \n",
        "\n",
        "def segnet_decoder_block(in_channels, out_channels):\n",
        "  \"\"\"Generate a SegNet decoder block ommitting the initial Upsampling.\n",
        "  \"\"\"\n",
        "  assert len(in_channels) == len(out_channels)\n",
        "  layers = [] \n",
        "  for i in range(len(in_channels)): \n",
        "    layer = segnet_decoder_layer(in_channels[i], out_channels[i])\n",
        "    layers.append(layer)\n",
        "  block = nn.Sequential(*layers)\n",
        "  return block"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUBowY0p4Ng-"
      },
      "source": [
        "#### SegNet class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CatAsvH3GTXs"
      },
      "source": [
        "class Segnet(nn.Module):\n",
        "  \"\"\"SegNet Class\"\"\"\n",
        "\n",
        "  def __init__(self, input_channels, output_channels, vgg16_bn):\n",
        "    \"\"\"Initialize an instance of SegNet\n",
        "\n",
        "      Args:\n",
        "        input_channels (int): number of input channels \n",
        "        output_channels (int): number of output channels\n",
        "        vgg16_bn (torch.model): pretrained VGG-16 (with Batch Normalization) model\n",
        "    \"\"\"\n",
        "    super(Segnet, self).__init__()\n",
        "    self.input_channels = input_channels \n",
        "    self.output_channels = output_channels    \n",
        "\n",
        "    # Encoder (VGG16 without Classifier)\n",
        "    self.enc_block00 = segnet_encoder_block(self.encoder_dims('block00','in'), self.encoder_dims('block00','out'))\n",
        "    self.enc_block01 = segnet_encoder_block(self.encoder_dims('block01','in'), self.encoder_dims('block01','out'))\n",
        "    self.enc_block02 = segnet_encoder_block(self.encoder_dims('block02','in'), self.encoder_dims('block02','out'))\n",
        "    self.enc_block03 = segnet_encoder_block(self.encoder_dims('block03','in'), self.encoder_dims('block03','out'))\n",
        "    self.enc_block04 = segnet_encoder_block(self.encoder_dims('block04','in'), self.encoder_dims('block04','out'))\n",
        "    \n",
        "    # Set pretrained weights \n",
        "    self._load_encoder_weights(vgg16_bn)\n",
        "\n",
        "    # Decoder \n",
        "    self.dec_block04 = segnet_decoder_block(self.decoder_dims('block04','in'), self.decoder_dims('block04','out'))\n",
        "    self.dec_block03 = segnet_decoder_block(self.decoder_dims('block03','in'), self.decoder_dims('block03','out'))\n",
        "    self.dec_block02 = segnet_decoder_block(self.decoder_dims('block02','in'), self.decoder_dims('block02','out'))\n",
        "    self.dec_block01 = segnet_decoder_block(self.decoder_dims('block01','in'), self.decoder_dims('block01','out'))\n",
        "    self.dec_block00 = segnet_decoder_block(self.decoder_dims('block00','in'), self.decoder_dims('block00','out'))\n",
        "\n",
        "    # Softmax\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Compute a forward pass \n",
        "\n",
        "      Args: \n",
        "        x (torch.Tensor): input image\n",
        "\n",
        "      Returns: \n",
        "        (torch.Tensor, torch.Tensor): output of the network without and applying softmax\n",
        "    \"\"\"\n",
        "    # -- Encoder --\n",
        "    # Encoder Block 00\n",
        "    dim00 = x.size()\n",
        "    enc00 = self.enc_block00(x)\n",
        "    mp00, indices00 = F.max_pool2d(enc00, kernel_size=2, stride=2, return_indices=True)\n",
        "    # Encoder Block 01\n",
        "    dim01 = mp00.size()\n",
        "    enc01 = self.enc_block01(mp00)\n",
        "    mp01, indices01 = F.max_pool2d(enc01, kernel_size=2, stride=2, return_indices=True)\n",
        "    # Encoder Block 02\n",
        "    dim02 = mp01.size()\n",
        "    enc02 = self.enc_block02(mp01)\n",
        "    mp02, indices02 = F.max_pool2d(enc02, kernel_size=2, stride=2, return_indices=True)\n",
        "    # Encoder Block 03\n",
        "    dim03 = mp02.size()\n",
        "    enc03 = self.enc_block02(mp02)\n",
        "    mp03, indices03 = F.max_pool2d(enc03, kernel_size=2, stride=2, return_indices=True)\n",
        "    # Encoder Block 04\n",
        "    dim04 = mp03.size()\n",
        "    enc04 = self.enc_block02(mp03)\n",
        "    mp04, indices04 = F.max_pool2d(enc04, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "    # -- Decoder --\n",
        "    # Decoder Block 04 \n",
        "    up04 = F.max_unpool2d(mp04, indices04, kernel_size=2, stride=2, output_size=dim04)\n",
        "    dec04 = self.dec_block04(up04)\n",
        "    # Decoder Block 03\n",
        "    up03 = F.max_unpool2d(dec04, indices03, kernel_size=2, stride=2, output_size=dim03)\n",
        "    dec03 = self.dec_block03(up03)\n",
        "    # Decoder Block 02\n",
        "    up02 = F.max_unpool2d(dec03, indices02, kernel_size=2, stride=2, output_size=dim02)\n",
        "    dec02 = self.dec_block03(up02)\n",
        "    # Decoder Block 01\n",
        "    up01 = F.max_unpool2d(dec02, indices01, kernel_size=2, stride=2, output_size=dim01)\n",
        "    dec01 = self.dec_block03(up01)\n",
        "    # Decoder Block 00\n",
        "    up00 = F.max_unpool2d(dec01, indices00, kernel_size=2, stride=2, output_size=dim00)\n",
        "    dec00 = self.dec_block03(up00)\n",
        "    # Softmax\n",
        "    output = self.softmax(dec00)\n",
        "\n",
        "    return dec00, output\n",
        "\n",
        "  def encoder_dims(self, block, io):\n",
        "    \"\"\"Obtain the encoder dimensions based on the input dimensions\n",
        "\n",
        "      Args: \n",
        "        block (string): encoder block \n",
        "        io (string): 'in' or 'out'\n",
        "\n",
        "      Returns: \n",
        "        (array): dimensions of the corresponding encoder block\n",
        "    \"\"\"\n",
        "    encoder_dimensions = {\n",
        "      'block00': { 'in': [self.input_channels, 64],'out': [64, 64] },\n",
        "      'block01': { 'in': [64, 128],       'out': [128, 128] },\n",
        "      'block02': { 'in': [128, 256, 256], 'out': [256, 256, 256] },\n",
        "      'block03': { 'in': [256, 512, 512], 'out': [512, 512, 512] },\n",
        "      'block04': { 'in': [512, 512, 512], 'out': [512, 512, 512] }\n",
        "    }\n",
        "    return encoder_dimensions[block][io]\n",
        "\n",
        "  def decoder_dims(self, block, io): \n",
        "    \"\"\"Obtain the decoder's dimensions based on the output dimensions \n",
        "    \"\"\"\n",
        "    decoder_dimensions = {\n",
        "      'block04': { 'in': [512, 512, 512], 'out': [512, 512, 512] },\n",
        "      'block03': { 'in': [512, 512, 512], 'out': [512, 512, 256] },\n",
        "      'block02': { 'in': [256, 256, 256], 'out': [256, 256, 128] },\n",
        "      'block01': { 'in': [128, 128],       'out': [128, 64] },\n",
        "      'block00': { 'in': [64, 64],         'out': [64, self.output_channels] }\n",
        "    }\n",
        "    return decoder_dimensions[block][io]\n",
        "    \n",
        "  def _load_encoder_weights(self, vgg16_bn):\n",
        "    \"\"\"\n",
        "      Load the corresponding weights of the train VGG16 model into the encoder.\n",
        "    \"\"\" \n",
        "    # Encoder block00\n",
        "    assert self.enc_block00[0][0].weight.size() == vgg16_bn.features[0].weight.size() \n",
        "    assert self.enc_block00[0][0].bias.size() == vgg16_bn.features[0].bias.size() \n",
        "    assert self.enc_block00[0][1].weight.size() == vgg16_bn.features[1].weight.size() \n",
        "    assert self.enc_block00[0][1].bias.size() == vgg16_bn.features[1].bias.size() \n",
        "    assert self.enc_block00[1][0].weight.size() == vgg16_bn.features[3].weight.size() \n",
        "    assert self.enc_block00[1][0].bias.size() == vgg16_bn.features[3].bias.size() \n",
        "    assert self.enc_block00[1][1].weight.size() == vgg16_bn.features[4].weight.size() \n",
        "    assert self.enc_block00[1][1].bias.size() == vgg16_bn.features[4].bias.size() \n",
        "\n",
        "    self.enc_block00[0][0].weight.data = vgg16_bn.features[0].weight.data \n",
        "    self.enc_block00[0][0].bias.data = vgg16_bn.features[0].bias.data \n",
        "    self.enc_block00[0][1].weight.data = vgg16_bn.features[1].weight.data \n",
        "    self.enc_block00[0][1].bias.data = vgg16_bn.features[1].bias.data \n",
        "    self.enc_block00[1][0].weight.data = vgg16_bn.features[3].weight.data \n",
        "    self.enc_block00[1][0].bias.data = vgg16_bn.features[3].bias.data \n",
        "    self.enc_block00[1][1].weight.data = vgg16_bn.features[4].weight.data \n",
        "    self.enc_block00[1][1].bias.data = vgg16_bn.features[4].bias.data\n",
        "\n",
        "    # Encoder block01\n",
        "    assert self.enc_block01[0][0].weight.size() == vgg16_bn.features[7].weight.size() \n",
        "    assert self.enc_block01[0][0].bias.size() == vgg16_bn.features[7].bias.size() \n",
        "    assert self.enc_block01[0][1].weight.size() == vgg16_bn.features[8].weight.size() \n",
        "    assert self.enc_block01[0][1].bias.size() == vgg16_bn.features[8].bias.size() \n",
        "    assert self.enc_block01[1][0].weight.size() == vgg16_bn.features[10].weight.size() \n",
        "    assert self.enc_block01[1][0].bias.size() == vgg16_bn.features[10].bias.size() \n",
        "    assert self.enc_block01[1][1].weight.size() == vgg16_bn.features[11].weight.size() \n",
        "    assert self.enc_block01[1][1].bias.size() == vgg16_bn.features[11].bias.size() \n",
        "    \n",
        "    self.enc_block01[0][0].weight.data = vgg16_bn.features[7].weight.data \n",
        "    self.enc_block01[0][0].bias.data = vgg16_bn.features[7].bias.data \n",
        "    self.enc_block01[0][1].weight.data = vgg16_bn.features[8].weight.data \n",
        "    self.enc_block01[0][1].bias.data = vgg16_bn.features[8].bias.data \n",
        "    self.enc_block01[1][0].weight.data = vgg16_bn.features[10].weight.data \n",
        "    self.enc_block01[1][0].bias.data = vgg16_bn.features[10].bias.data \n",
        "    self.enc_block01[1][1].weight.data = vgg16_bn.features[11].weight.data \n",
        "    self.enc_block01[1][1].bias.data = vgg16_bn.features[11].bias.data \n",
        "\n",
        "    # Encoder block02 \n",
        "    assert self.enc_block02[0][0].weight.size() == vgg16_bn.features[14].weight.size() \n",
        "    assert self.enc_block02[0][0].bias.size() == vgg16_bn.features[14].bias.size() \n",
        "    assert self.enc_block02[0][1].weight.size() == vgg16_bn.features[15].weight.size() \n",
        "    assert self.enc_block02[0][1].bias.size() == vgg16_bn.features[15].bias.size() \n",
        "    assert self.enc_block02[1][0].weight.size() == vgg16_bn.features[17].weight.size() \n",
        "    assert self.enc_block02[1][0].bias.size() == vgg16_bn.features[17].bias.size() \n",
        "    assert self.enc_block02[1][1].weight.size() == vgg16_bn.features[18].weight.size() \n",
        "    assert self.enc_block02[1][1].bias.size() == vgg16_bn.features[18].bias.size() \n",
        "    assert self.enc_block02[2][0].weight.size() == vgg16_bn.features[20].weight.size() \n",
        "    assert self.enc_block02[2][0].bias.size() == vgg16_bn.features[20].bias.size() \n",
        "    assert self.enc_block02[2][1].weight.size() == vgg16_bn.features[21].weight.size() \n",
        "    assert self.enc_block02[2][1].bias.size() == vgg16_bn.features[21].bias.size() \n",
        "\n",
        "    self.enc_block02[0][0].weight.data = vgg16_bn.features[14].weight.data \n",
        "    self.enc_block02[0][0].bias.data = vgg16_bn.features[14].bias.data \n",
        "    self.enc_block02[0][1].weight.data = vgg16_bn.features[15].weight.data \n",
        "    self.enc_block02[0][1].bias.data = vgg16_bn.features[15].bias.data \n",
        "    self.enc_block02[1][0].weight.data = vgg16_bn.features[17].weight.data \n",
        "    self.enc_block02[1][0].bias.data = vgg16_bn.features[17].bias.data \n",
        "    self.enc_block02[1][1].weight.data = vgg16_bn.features[18].weight.data \n",
        "    self.enc_block02[1][1].bias.data = vgg16_bn.features[18].bias.data \n",
        "    self.enc_block02[2][0].weight.data = vgg16_bn.features[20].weight.data \n",
        "    self.enc_block02[2][0].bias.data = vgg16_bn.features[20].bias.data \n",
        "    self.enc_block02[2][1].weight.data = vgg16_bn.features[21].weight.data \n",
        "    self.enc_block02[2][1].bias.data = vgg16_bn.features[21].bias.data \n",
        "\n",
        "    # Encoder block03\n",
        "    assert self.enc_block03[0][0].weight.size() == vgg16_bn.features[24].weight.size() \n",
        "    assert self.enc_block03[0][0].bias.size() == vgg16_bn.features[24].bias.size() \n",
        "    assert self.enc_block03[0][1].weight.size() == vgg16_bn.features[25].weight.size() \n",
        "    assert self.enc_block03[0][1].bias.size() == vgg16_bn.features[25].bias.size()\n",
        "    assert self.enc_block03[1][0].weight.size() == vgg16_bn.features[27].weight.size() \n",
        "    assert self.enc_block03[1][0].bias.size() == vgg16_bn.features[27].bias.size() \n",
        "    assert self.enc_block03[1][1].weight.size() == vgg16_bn.features[28].weight.size() \n",
        "    assert self.enc_block03[1][1].bias.size() == vgg16_bn.features[28].bias.size() \n",
        "    assert self.enc_block03[2][0].weight.size() == vgg16_bn.features[30].weight.size() \n",
        "    assert self.enc_block03[2][0].bias.size() == vgg16_bn.features[30].bias.size() \n",
        "    assert self.enc_block03[2][1].weight.size() == vgg16_bn.features[31].weight.size() \n",
        "    assert self.enc_block03[2][1].bias.size() == vgg16_bn.features[31].bias.size() \n",
        "\n",
        "    self.enc_block03[0][0].weight.data = vgg16_bn.features[24].weight.data \n",
        "    self.enc_block03[0][0].bias.data = vgg16_bn.features[24].bias.data \n",
        "    self.enc_block03[0][1].weight.data = vgg16_bn.features[25].weight.data \n",
        "    self.enc_block03[0][1].bias.data = vgg16_bn.features[25].bias.data\n",
        "    self.enc_block03[1][0].weight.data = vgg16_bn.features[27].weight.data \n",
        "    self.enc_block03[1][0].bias.data = vgg16_bn.features[27].bias.data \n",
        "    self.enc_block03[1][1].weight.data = vgg16_bn.features[28].weight.data \n",
        "    self.enc_block03[1][1].bias.data = vgg16_bn.features[28].bias.data \n",
        "    self.enc_block03[2][0].weight.data = vgg16_bn.features[30].weight.data \n",
        "    self.enc_block03[2][0].bias.data = vgg16_bn.features[30].bias.data \n",
        "    self.enc_block03[2][1].weight.data = vgg16_bn.features[31].weight.data \n",
        "    self.enc_block03[2][1].bias.data = vgg16_bn.features[31].bias.data \n",
        "\n",
        "    # Encoder block04\n",
        "    assert self.enc_block04[0][0].weight.size() == vgg16_bn.features[34].weight.size() \n",
        "    assert self.enc_block04[0][0].bias.size() == vgg16_bn.features[34].bias.size() \n",
        "    assert self.enc_block04[0][1].weight.size() == vgg16_bn.features[35].weight.size() \n",
        "    assert self.enc_block04[0][1].bias.size() == vgg16_bn.features[35].bias.size() \n",
        "    assert self.enc_block04[1][0].weight.size() == vgg16_bn.features[37].weight.size() \n",
        "    assert self.enc_block04[1][0].bias.size() == vgg16_bn.features[37].bias.size() \n",
        "    assert self.enc_block04[1][1].weight.size() == vgg16_bn.features[38].weight.size() \n",
        "    assert self.enc_block04[1][1].bias.size() == vgg16_bn.features[38].bias.size() \n",
        "    assert self.enc_block04[2][0].weight.size() == vgg16_bn.features[40].weight.size() \n",
        "    assert self.enc_block04[2][0].bias.size() == vgg16_bn.features[40].bias.size() \n",
        "    assert self.enc_block04[2][1].weight.size() == vgg16_bn.features[41].weight.size() \n",
        "    assert self.enc_block04[2][1].bias.size() == vgg16_bn.features[41].bias.size()\n",
        "\n",
        "    self.enc_block04[0][0].weight.data = vgg16_bn.features[34].weight.data \n",
        "    self.enc_block04[0][0].bias.data = vgg16_bn.features[34].bias.data \n",
        "    self.enc_block04[0][1].weight.data = vgg16_bn.features[35].weight.data \n",
        "    self.enc_block04[0][1].bias.data = vgg16_bn.features[35].bias.data \n",
        "    self.enc_block04[1][0].weight.data = vgg16_bn.features[37].weight.data \n",
        "    self.enc_block04[1][0].bias.data = vgg16_bn.features[37].bias.data \n",
        "    self.enc_block04[1][1].weight.data = vgg16_bn.features[38].weight.data \n",
        "    self.enc_block04[1][1].bias.data = vgg16_bn.features[38].bias.data \n",
        "    self.enc_block04[2][0].weight.data = vgg16_bn.features[40].weight.data \n",
        "    self.enc_block04[2][0].bias.data = vgg16_bn.features[40].bias.data \n",
        "    self.enc_block04[2][1].weight.data = vgg16_bn.features[41].weight.data \n",
        "    self.enc_block04[2][1].bias.data = vgg16_bn.features[41].bias.data\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFzbRkhX328b"
      },
      "source": [
        "local_path = 'VOCdevkit2/VOC2012/' # modify it according to your device\n",
        "bs = 12\n",
        "epochs =  1"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb1Z77zX328b"
      },
      "source": [
        "### 1.4 Dataset and Dataloader(0.5 points)\n",
        "Create the dataset using pascalVOCDataset class defined above. Use local_path defined in the cell above as root. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVj1nGEk328c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "638f95cb-731c-495a-e369-2a652d802e81"
      },
      "source": [
        "# dataset variable\n",
        "training_data = pascalVOCDataset(local_path, split=\"train\", is_transform=True)\n",
        "validation_data = pascalVOCDataset(local_path, split=\"val\", is_transform=True)\n",
        "\n",
        "print(\"Training data instances: \", len(training_data))\n",
        "print(\"Validation data instances:\", len(validation_data))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-5c54e25ddefe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dataset variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpascalVOCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpascalVOCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training data instances: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f0ac50271016>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, sbd_path, split, is_transform, img_size, augmentations, img_norm, test_mode)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trainval\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ImageSets/Segmentation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VOCdevkit2/VOC2012/ImageSets/Segmentation/train.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POXshZpj3j9j"
      },
      "source": [
        "trainloader = data.DataLoader(training_data, batch_size=bs, shuffle=True, num_workers=0)\n",
        "valloader = data.DataLoader(validation_data, batch_size=bs, shuffle=True, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgAzydTc4Ng8"
      },
      "source": [
        "### Load the pretrained VGG16 model (with batch normalization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7IPrATF4Ng9",
        "outputId": "60c6a009-1121-4ecc-e06c-eaa9e21d183e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#if os.path.isdir('models/vgg16_bn/hub/checkpoints'):\n",
        "#    checkpoint = torch.load('models/vgg16_bn/hub/checkpoints/vgg16_bn-6c64b313.pth')\n",
        "#    vgg16_bn = models.vgg16_bn()\n",
        "#    vgg16_bn.load_state_dict(checkpoint)\n",
        "#else:\n",
        "#    os.environ['TORCH_HOME'] = 'models/vgg16_bn'\n",
        "vgg16_bn = models.vgg16_bn(pretrained=True, progress=True)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-832f1434d163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/vgg16_bn/hub/checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/vgg16_bn/hub/checkpoints/vgg16_bn-6c64b313.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvgg16_bn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvgg16_bn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mvgg16_bn\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplays\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0mbar\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg16_bn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36m_vgg\u001b[0;34m(arch, cfg, batch_norm, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'init_weights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         state_dict = load_state_dict_from_url(model_urls[arch],\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, features, num_classes, init_weights)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         self.classifier = nn.Sequential(\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfQiOnEkGZat"
      },
      "source": [
        "# Creating an instance of the model defined above. \n",
        "# You can modify it incase you need to pass paratemers to the constructor.\n",
        "model = Segnet(3, 21, vgg16_bn)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXGaI5DX328c"
      },
      "source": [
        "### 1.5 Loss fuction and Optimizer(1.0 point)\n",
        "Define below with the loss function you think would be most suitable for segmentation task. You are free to choose any optimizer to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd2FT1Il328c"
      },
      "source": [
        "# loss function\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer variable\n",
        "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GeCkXip328c"
      },
      "source": [
        "### 1.6 Training the model(3.0 points)\n",
        "Your task here is to complete the code below to perform a training loop and save the model weights after each epoch of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuNg5Ak74NhC",
        "outputId": "6ab6462d-7a8c-4c5c-e462-69eb4bfcdd5d"
      },
      "source": [
        "input, labels = next(iter(trainloader))\n",
        "input.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 3, 512, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF7rxT4d4NhC"
      },
      "source": [
        "output = model.forward(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtrXN22B4NhD"
      },
      "source": [
        "output.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz08hSdPKODm"
      },
      "source": [
        "for _ in range(epochs):\n",
        "  for i, d in enumerate(trainloader):\n",
        "    # your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tXAA4jQ328d"
      },
      "source": [
        "### 1.7 Evaluate your model(1.5 points)\n",
        "In this section you have to implement the evaluation metrics for your model. Calculate the values of F1-score, dice coefficient and AUC-ROC score on the data you used for training. You can use external packages like scikit-learn to compute above metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyVBhE_K328d"
      },
      "source": [
        "def evaluate(ground_truth, predictions):\n",
        "    \n",
        "    return f1_score, auc_score, dice_coeeficient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GimQrA6S328d"
      },
      "source": [
        "### 1.8 Plot the evaluation metrics against epochs(1.0)\n",
        "In section 1.6 we saved the weights of the model after each epoch. In this section, you have to calculate the evaluation metrics after each epoch of training by loading the weights for each epoch. Once you have calculated the evaluation metrics for each epoch, plot them against the epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9lEY66w328d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqzsNJHz328e"
      },
      "source": [
        "### 1.9 Visualize results(0.5 points)\n",
        "For any 10 images in the dataset, show the images along the with their segmentation mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NS50IL_c7Mf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}